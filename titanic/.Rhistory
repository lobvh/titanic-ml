install.packages("randomForest")
setwd("~/Desktop/git_projects/titanic-ml/titanic")
library(doSNOW)
library(rpart)
library(rpart.plot)
library(randomForest)
library(stringr)
library(ggplot2)
library(caret)
train_data <- "train.csv"
test_data <- "test.csv"
#Load the data
train_set <- read.csv(train_data, header = TRUE) #We want the column names to be in the header, so header = TRUE
test_set <- read.csv(test_data, header = TRUE)
#Make combined dataset
survived <- rep("None", nrow(test_set))
test_set_with_survived <- data.frame(survived, test_set)
data_combined <- rbind(train_set, test_set_with_survived)
#Convert pclass and survived as pclass
data_combined$survived <- as.factor(data_combined$survived)
data_combined$pclass <- as.factor(data_combined$pclass)
####
#We will use whole dataset but different features for each Random Forest algorithm.
#Note that on combined dataset labels are "1" "0" and "None", where in train dataset we already have preprocessed ones and zeroes.
####
RF_LABELS <- as.factor(train_set$survived) #Made it upper-case since it's constant.
set.seed(2348)
cv_10_folds <- createMultiFolds(RF_LABELS, k = 10, times = 10)
# Set up caret's trainControl object per above.
ctrl_1 <- trainControl(method = "repeatedcv", number = 10, repeats = 10,
index = cv_10_folds)
####
#Past titles
####
extractTitle <- function(name) {
name <- as.character(name)
if (length(grep("Miss.", name)) > 0) {
return ("Miss.")
} else if (length(grep("Master.", name)) > 0) {
return ("Master.")
} else if (length(grep("Mrs.", name)) > 0) {
return ("Mrs.")
} else if (length(grep("Mr.", name)) > 0) {
return ("Mr.")
} else {
return ("Other")
}
}
titles <- NULL
for (i in 1:nrow(data_combined)) {
titles <- c(titles, extractTitle(data_combined[i,"name"]))
}
data_combined$title <- as.factor(titles)
#####
#New Titles
#####
name.splits <- str_split(data_combined$name, ",")
name.splits <- str_split(sapply(name.splits, "[", 2), " ")
titles <- sapply(name.splits, "[", 2)
# Re-map titles to be more exact
titles[titles %in% c("Dona.", "the")] <- "Lady."
titles[titles %in% c("Ms.", "Mlle.")] <- "Miss."
titles[titles == "Mme."] <- "Mrs."
titles[titles %in% c("Jonkheer.", "Don.")] <- "Sir."
titles[titles %in% c("Col.", "Capt.", "Major.")] <- "Officer"
table(titles)
# Make title a factor
data_combined$new.title <- as.factor(titles)
####
#Collapse titles based on visual analysis
####
indexes <- which(data_combined$new.title == "Lady.")
data_combined$new.title[indexes] <- "Mrs."
indexes <- which(data_combined$new.title == "Dr." |
data_combined$new.title == "Rev." |
data_combined$new.title == "Sir." |
data_combined$new.title == "Officer")
data_combined$new.title[indexes] <- "Mr."
#####
#Ticket party size and Average fare feature
#####
ticket.party.size <- rep(0, nrow(data_combined))
avg.fare <- rep(0.0, nrow(data_combined))
tickets <- unique(data_combined$ticket)
for (i in 1:length(tickets)) {
current.ticket <- tickets[i]
party.indexes <- which(data_combined$ticket == current.ticket)
current.avg.fare <- data_combined[party.indexes[1], "fare"] / length(party.indexes)
for (k in 1:length(party.indexes)) {
ticket.party.size[party.indexes[k]] <- length(party.indexes)
avg.fare[party.indexes[k]] <- current.avg.fare
}
}
data_combined$ticket.party.size <- ticket.party.size
data_combined$avg.fare <- avg.fare
View(data_combined)
features <- c("pclass", "new.title", "ticket.party.size", "avg.fare")
xgboost_train_data <- cbind(data_combined[1:891, features], survived = RF_LABELS)
summary(xgboost_train_data$new.title)
xgboost_train_data$new.title <- as.character(xgboost_train_data$new.title)
summary(xgboost_train_data$new.title)
View(xgboost_train_data)
xgboost_train_data$new.title <- as.factor(xgboost_train_data$new.title)
summary(xgboost_train_data$new.title)
train.control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
search = "grid")
####
#These are found on the Internet. I know how in an essence XGBoost works, but I like the way professionals think.
####
tune.grid <- expand.grid(eta = c(0.05, 0.075, 0.1),
nrounds = c(50, 75, 100),
max_depth = 6:8,
min_child_weight = c(2.0, 2.25, 2.5),
colsample_bytree = c(0.3, 0.4, 0.5),
gamma = 0,
subsample = 1)
cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)
xgboost_train <- train(survived ~ .,
data = xgboost_train_data,
method = "xgbTree", tuneGrid = tune.grid, trControl = train.control)
stopCluster(cl)
library(doSNOW)
library(rpart)
library(rpart.plot)
library(randomForest)
library(stringr)
library(ggplot2)
library(caret)
cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)
xgboost_train <- train(survived ~ .,
data = xgboost_train_data,
method = "xgbTree", tuneGrid = tune.grid, trControl = train.control)
stopCluster(cl)
train.control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
search = "grid")
####
#These are found on the Internet. I know how in an essence XGBoost works, but I like the way professionals think.
####
tune.grid <- expand.grid(eta = c(0.05, 0.075, 0.1),
nrounds = c(50, 75, 100),
max_depth = 6:8,
min_child_weight = c(2.0, 2.25, 2.5),
colsample_bytree = c(0.3, 0.4, 0.5),
gamma = 0,
subsample = 1)
cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)
xgboost_train <- train(survived ~ .,
data = xgboost_train_data,
method = "xgbTree", tuneGrid = tune.grid, trControl = train.control)
stopCluster(cl)
saveRDS(xgboost_train, "xgboost_model_1.rds")
xg_boost_model <- readRDS("xgboost_model_1.rds")
test_submit_data <- data_combined[892:1309, features]
test_submit_data$new.title <- as.character(test_submit_data$new.title)
test_submit_data$new.title <- as.factor(test_submit_data$new.title)
xg_boost_preds <- predict(xg_boost_model, test_submit_data)
submit_df <- data.frame(PassengerId = rep(892:1309), Survived = xg_boost_preds)
View(xgboost_train_data)
