ggplot(data_combined[1:891,], aes(x = family_size, fill = survived)) +
geom_bar() +
facet_wrap(~pclass + title) +
ggtitle("Pclass, Title") +
xlab("family.size") +
ylab("Total Count") +
ylim(0,300) +
labs(fill = "Survived")
###
#It's much informative in term of "survival trend" among the classes and titles, and maybe even obvious that
#if you have large family and it might be problem for you to keep everyone around, thus the chances of survival are bad.
###
# - - - ticket - - -
#####
#Just before we start I want to emphasise something called overfiting.
#I would be precise and say "overfiting on training data" since we are building our model based on training data.
#We could make a model that could be 100% precise, for example "if you are that and that name, you survive/perish" and build a model on top of that.
#On top of 'name' variables. When we imput some new data (test data) and there is no data with that name in the training set we are ruined.
#That is why we need a model that GENERALIZES on all training data. So when you see those graphs that map each data point precisely, that is the sign of overfitting.
#####
####
#On to the ticket variable. As Dave pointed out, one should get used to write str() for each variable whenever we start analyzing it.
#It's a shorthand for using str() on a combined dataset.
####
str(data_combined$ticket)
####
#Based on the huge number of levels ticket really isn't a factor variable it is a string, so let's convert it.
#Then we will show first 50 of them. We could use 'any number'. It's for a sake of giving us a bigger picture aka we will better see pattern
#if there are relatively high number of data points, especially for this kind of variable.
####
data_combined$ticket <- as.character(data_combined$ticket)
data_combined$ticket[1:50]
####
#There's no immediately apparent structure in the data, let's see if we can find some. I saw on the internet that those indicate something but won't delve deep into that.
#We'll start with taking a look at just the first char for each.
#It's easy for visualization if there are relatively few (we can make them as factor!)
#It's not necessarily correct way of extracting feature, or that one should expect something out of it.
#This is some 'basic heuristic'.
####
ticket_first_char <- ifelse(data_combined$ticket == "", " ", substr(data_combined$ticket,1,1))
unique(ticket_first_char)
####
#OK, we can make a factor for analysis purposes and visualize
###
data_combined$ticket_first_char <- as.factor(ticket_first_char)
# First, a high-level plot of the data
ggplot(data_combined[1:891,], aes(x = ticket_first_char, fill = survived)) +
geom_bar() +
ggtitle("Survivability by ticket.first.char") +
xlab("ticket.first.char") +
ylab("Total Count") +
ylim(0,350) +
labs(fill = "Survived")
####
#One might conclude from the plot that since we know that those tickets that start with 1, 2 and 3 might be indicators of pclass.
#But there are more people in first class that in second:
table(data_combined$pclass)
#Maybe those from 4, 5, etc. could be also grouped into first, second and third class in order to confirm previous hypothesis?
#It's quite a mixed bag of survivability, and we hate overfiting... Might be predictive, but let's drill a bit more!
####
ggplot(data_combined[1:891,], aes(x = ticket_first_char, fill = survived)) +
geom_bar() +
facet_wrap(~pclass) +
ggtitle("Pclass") +
xlab("ticket.first.char") +
ylab("Total Count") +
ylim(0,300) +
labs(fill = "Survived")
###
#We can see that majority tickets in each class start with the same number as pclass they are in.
#If we put that stack of P on top of 1 in the first class we will have "that amount of people" which
#further implies that our previous hypothesis is true! That majority of imbalance for first class is "hidding" in the P?
#There is W in each class, and very few of them. Might indicate some kind of people that are indeed passengers, but maybe some workers or something?
#There might be some signal here, but I will keep it in back of my head.
###
####
# Lastly, see if we get a pattern when using combination of pclass & title
###
ggplot(data_combined[1:891,], aes(x = ticket_first_char, fill = survived)) +
geom_bar() +
facet_wrap(~pclass + title) +
ggtitle("Pclass, Title") +
xlab("ticket.first.char") +
ylab("Total Count") +
ylim(0,200) +
labs(fill = "Survived")
###
#Again, this is based just on a frist letter of ticket. Everything matches with our intuition about survivability, but I don't
#see some patterns that stick out here. Maybe thorough investigation of ticket feature will be more fruitful.
###
# - - - fare - - -
####
#Next up - the fares Titanic passengers paid
####
str(data_combined$fare)
summary(data_combined$fare)
length(unique(data_combined$fare))
###
#We can't make it a factor... Too much instances.
###
###High level overview
ggplot(data_combined, aes(x = fare)) +
geom_histogram(binwidth = 5) +
ggtitle("Combined Fare Distribution") +
xlab("Fare") +
ylab("Total Count") +
ylim(0,200)
####
#There are some folks that haven't paid anything? That could be interesting.
#We see that distribution is skewed towards higher end. We should expect that, because majority of folks that were on Titanic were in third class.
#aka low fares. We confirmed that with median<mean, and also median = 14.454.
#There is an outlier up there where fare>500, but further investigation of fare variable will tell us why is that so.
###
####
#Now that we have high level overview, let's see if it's predictive in some sense:
####
ggplot(data_combined[1:891,], aes(x = fare, fill = survived)) +
geom_histogram(binwidth = 5) +
facet_wrap(~pclass + title) +
ggtitle("Pclass, Title") +
xlab("fare") +
ylab("Total Count") +
ylim(0,50) +
labs(fill = "Survived")
####
#One would expect that rich folks survived more, and indeed that is so. I don't see anything that will cause more than overfitting here, because
#everytihing matches our previous intuitions in terms of survivability. I will leave fare for now, but it might help me to feature engineer something using it.
####
################################################################################################
#                               EXPLANATORY DATA ANALYSIS                                      #
################################################################################################
####
#
#This is the part where we check our intuitions about features, and also test if our feature engineering is worthwhile.
#I think "all" of the ML algos have explicit or implicit way of providing us with the feature importance.
#We need something that is fast (for classification problems!), effective and simple to interpret.
#Without worrying too much on hyperparameters. We will leave that fine tuning for the "real" modeling part.
#
#
#We will use Random Forests here. I wont drill about the algorithm here there is plenty of it on Internet.
#In an essence it uses ensemble method of trees and averages loss on each tree.
#Each tree gets it bootstraped sample (drawing N samples from training set where N is number of rows in training set).
#By drawing samples with replacement some of the rows won't be sampled. Those are colled out-of-bag.
#Evaluate model on samples that you drawed, test the model on non-sampled (out-of-bag) ones.
#What confuses most people is when you are evaluating loss on that particular tree OOB sample is HIDDING it's labels.
#You get the predicted ones, then you compare it with the real labels and calculate loss.
#Draw samples -> Make model (tree) on them -> Test the model (tree) on OOB by hiding it's labels -> Compare real labels of OOB with predicted ones -> Calculate loss
#Then you repeat that proces for each tree and compute the average loss.
#Now watch out, since you are drawing samples RANDOMLY with replacement some of the OOB's from the first tree won't be OOB on some other trees.
#That's why validation on test data is way more accurate of overall accuracy than OOB since each data point from test set has been put
#through EACH TREE in Random Forest, where some of the OOB samples are not tested on all trees.
#There is also (I guess!) a high probability that EACH training sample will be picked in some of the OOB's at least once, so that's why in confusion matrix you will get score for each row.
#
#That's a high-overview enough to get you started. I'm not an expert on RF's...
#
####
###
#Let's import the randomForest library.
###
library(randomForest)
####
#We will use whole dataset but different features for each Random Forest algorithm.
#Note that on combined dataset labels are "1" "0" and "None", where in train dataset we already have preprocessed ones and zeroes.
####
RF_LABELS <- as.factor(train_set$survived) #Made it upper-case since it's constant.
library(caret)
library(doSNOW)
set.seed(37596)
cv_3_folds <- createMultiFolds(RF_LABELS, k = 3, times = 10)
ctrl_3 <- trainControl(method = "repeatedcv", number = 3, repeats = 10,
index = cv_3_folds)
library(rpart)
library(rpart.plot)
####
#This is sort of like 'reverse engineering'. We will see that not only that tree will train on all the features we give it,
#but will also 'drop' ones that don't have too much information, which is better than RF's feature importance.
#Since 'if you repeat same stuff again, you should write a function' holds, we will make one for training trees.
####
rpart.cv <- function(seed, training, labels, ctrl) {
cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)
set.seed(seed)
# Leverage formula interface for training
rpart.cv <- train(x = training, y = labels, method = "rpart", tuneLength = 30,
trControl = ctrl)
#Shutdown cluster
stopCluster(cl)
return (rpart.cv)
}
####
#Let's grab the 'most promising features' to inspect what got wrong, and what are the potential causes of overfitting.
####
features <- c("pclass", "title", "family_size")
rpart.train.1 <- data_combined[1:891, features]
# Run CV and check out results
rpart.1.cv.1 <- rpart.cv(94622, rpart.train.1, RF_LABELS, ctrl_3)
rpart.1.cv.1
####
#We obtained the accuracy of 0.8210999 on this model.
#Compared to random forest model with the same parameters (0.8139169) we are better. But this is expected, since we are training decision tree.
####
#Let's plot the decision tree:
####
prp(rpart.1.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
####
#One can see from the tree where there might be problems. Some of the 'hypotheses' are also valid so things like
#'Have title of Miss, Mr., or Master. not and being in class 3 means that you will survive.
#But the decision tree intuited that if you are in pclass=3 it depends on your family size weather you survived or not.
#Since family_size = 5,6,8,11 is a bit too specific that might be overfitting, and we should concentrate on that part to improve it.
#Having the title Mr and Other means that you will die around 80% of time.
#There might be some women in title Other so we will investigate that in order to put everyone in the same basket.
#We know that title of Mr. have better survival rate at first class, so we also might improve that.
#
#So not only that tree's help us seeing where we might overfit, but also where we might improve our model.
#Intuitevely, if we find such subset of features to improve single tree, we are also gonna improve our overall model.
#So, using trees is a bit of a 'improve single, to improve overall'.
###
################################################################################################
#                             FIXING THE TITLE VARIABLE                                        #
################################################################################################
####
#Parse out last name and title:
#Based on how 'name' variable look we need to find a way to extract the title variable properly.
####
data_combined[1:25, "name"]
name.splits <- str_split(data_combined$name, ",")
name.splits[1]
####
#This might be a nice place to extract last name title, which might be used later.
####
last.names <- sapply(name.splits, "[", 1)
last.names[1:10]
data.combined$last.name <- last.names
####
#Here we are gonna extract titles.
####
name.splits <- str_split(sapply(name.splits, "[", 2), " ")
titles <- sapply(name.splits, "[", 2)
unique(titles)
####
#We see that some of the 'new' titles we havent seen might be indicative of nobility, and hence people who are rich.
#Using the experience of Google you can get the idea of why one would aggregate these variables as such:
####
# What's up with a title of 'the'?
data_combined[which(titles == "the"),]
# Re-map titles to be more exact
titles[titles %in% c("Dona.", "the")] <- "Lady."
titles[titles %in% c("Ms.", "Mlle.")] <- "Miss."
titles[titles == "Mme."] <- "Mrs."
titles[titles %in% c("Jonkheer.", "Don.")] <- "Sir."
titles[titles %in% c("Col.", "Capt.", "Major.")] <- "Officer"
table(titles)
# Make title a factor
data_combined$new.title <- as.factor(titles)
# Visualize new version of title
ggplot(data_combined[1:891,], aes(x = new.title, fill = survived)) +
geom_bar() +
facet_wrap(~ pclass) +
ggtitle("Surival Rates for new.title by pclass")
####
#We see that yeah, these folks wiht title of Dr. etc. are more correlated with those that are in higher classes.
#We have already decided that Ms, Mile are Miss, and Mme. is Mr.
#From this plot, based on relative survival rate, we can put into the same basket other
#nobility titles.
####
#Collapse titles based on visual analysis
####
indexes <- which(data_combined$new.title == "Lady.")
data_combined$new.title[indexes] <- "Mrs."
indexes <- which(data_combined$new.title == "Dr." |
data_combined$new.title == "Rev." |
data_combined$new.title == "Sir." |
data_combined$new.title == "Officer")
data_combined$new.title[indexes] <- "Mr."
#####
#Visualize the final result to check if everything is correct:
#####
ggplot(data_combined[1:891,], aes(x = new.title, fill = survived)) +
geom_bar() +
facet_wrap(~ pclass) +
ggtitle("Surival Rates for Collapsed new.title by pclass")
#####
#Since we made new.title variable, we will now train decision tree based on it to check
#if our intuition about "improving" accuracy is any good.
#####
features <- c("pclass", "new.title", "family_size")
rpart.train.2 <- data_combined[1:891, features]
# Run CV and check out results
rpart.2.cv.1 <- rpart.cv(94622, rpart.train.2, RF_LABELS, ctrl_3)
rpart.2.cv.1
####
#0.8285073 > 0.8210999, let's check how the decision tree splited the data.
####
# Plot
prp(rpart.2.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
####
#Compared to the previous tree we see that some of the "females" found in previous tree in "Others"
#is now got into right bucket. But we see that everything remains the same: family_size is overspecific, and Mr. are still "unlucky".
####
################################################################################################
#                             TRYING TO FIND THOSE MR-S WHO SURVIVED                           #
################################################################################################
######
#Dive in on 1st class "Mr.". We know that those in first class should have a higher survival rate...
#####
indexes.first.mr <- which(data_combined$new.title == "Mr." & data_combined$pclass == "1")
first.mr.df <- data_combined[indexes.first.mr, ]
summary(first.mr.df)
####
#One female? Let's check that out.
####
first.mr.df[first.mr.df$sex == "female",]
####
#We presumed that those with title of Dr. are ought to be males, mainly...
####
# Update new.title feature
indexes <- which(data_combined$new.title == "Mr." &
data_combined$sex == "female")
data_combined$new.title[indexes] <- "Mrs."
# Any other gender slip-ups?
length(which(data_combined$sex == "female" &
(data_combined$new.title == "Master." |
data_combined$new.title == "Mr.")))
# Refresh data frame
indexes.first.mr <- which(data_combined$new.title == "Mr." & data_combined$pclass == "1")
first.mr.df <- data_combined[indexes.first.mr, ]
# Let's look at surviving 1st class "Mr."
summary(first.mr.df[first.mr.df$survived == "1",])
View(first.mr.df[first.mr.df$survived == "1",])
#####
#The distribution of fare amongst males who were in first class is fairly variable.
####
# Take a look at some of the high fares
indexes <- which(data_combined$ticket == "PC 17755" |
data_combined$ticket == "PC 17611" |
data_combined$ticket == "113760")
View(data_combined[indexes,])
####
#One may intuit that we can now form "groups" based on the ticket feature, and based on that
#that those fares need to be corrected. They are same here, but one would not expect that each group member will pay the same amount of money.
#To find in proportion how much each of the family member "share" as an average Dave used the 'normal' average of fare per group where each member of family is weighted the same.
#Since all "ML" is around the idea "learning weights and biases", I thought, well there may be some algorithm that will learn proper weights (and biases?)
#that will make fare more realistic.
#I also asked the question if that is reasonable, so follow it up:
# https://www.kaggle.com/c/titanic/discussion/143678
#Because here I will use "normal" average.
####
# Visualize survival rates for 1st class "Mr." by fare
ggplot(first.mr.df, aes(x = fare, fill = survived)) +
geom_density(alpha = 0.5) +
ggtitle("1st Class 'Mr.' Survival Rates by fare")
####
#As I've pointed out earlier, when we use density plots we are mainly looking for the "clean separations".
#We see that those who didn't survive where the fare is >200 we expected those kind of folks to "live",
#and since there is not so much Mr's who survived with the fare >500, fare may not be helpful in finding first class Mr's who survived.
####
####
#Okay, we will engineer features based on all the passengers with the same ticket: use "proper fare" and make new feature
#which will be indicative of "group size". We will see if that might be predictive.
####
ticket.party.size <- rep(0, nrow(data_combined))
avg.fare <- rep(0.0, nrow(data_combined))
tickets <- unique(data_combined$ticket)
for (i in 1:length(tickets)) {
current.ticket <- tickets[i]
party.indexes <- which(data_combined$ticket == current.ticket)
current.avg.fare <- data_combined[party.indexes[1], "fare"] / length(party.indexes)
for (k in 1:length(party.indexes)) {
ticket.party.size[party.indexes[k]] <- length(party.indexes)
avg.fare[party.indexes[k]] <- current.avg.fare
}
}
data_combined$ticket.party.size <- ticket.party.size
data_combined$avg.fare <- avg.fare
# Refresh 1st class "Mr." dataframe
first.mr.df <- data_combined[indexes.first.mr, ]
summary(first.mr.df)
####
#Looking at the summary of new features we see that 75% of Mr-s in first class had group size less than 2.
#We might expect that data is now slightly skewed based on avg.fare, but there are also outliers.
#Let's visualize Mr's with new features to see if they might help us with the separations.
####
# Visualize new features
ggplot(first.mr.df[first.mr.df$survived != "None",], aes(x = ticket.party.size, fill = survived)) +
geom_density(alpha = 0.5) +
ggtitle("Survival Rates 1st Class 'Mr.' by ticket.party.size")
ggplot(first.mr.df[first.mr.df$survived != "None",], aes(x = avg.fare, fill = survived)) +
geom_density(alpha = 0.5) +
ggtitle("Survival Rates 1st Class 'Mr.' by avg.fare")
####
#Concerning the survival rates by ticket.party.size we see some blips apart from some regions (intervals) where based on the relative
#area there might be more of those who survived than those who don't and vice versa.
#Concerning the sruvival rates by avg.fare there is a nice "blip" in the region where avg.fare is between 25 and 30ish.
#The area for those who survied is much higher and narrower than those who died.
#We know from statistics that narrower the distribution is, the measurment is "precise".
####
# Hypothesis - ticket.party.size is highly correlated with avg.fare
summary(data_combined$avg.fare)
# One missing value, take a look
data_combined[is.na(data_combined$avg.fare), ]
?with
data_combined[is.na(data_combined$avg.fare), ]
# Get records for similar passengers and summarize avg.fares
indexes <- with(data_combined, which(pclass == "3" & title == "Mr." & family.size == 1 &
ticket != "3701"))
similar.na.passengers <- data_combined[indexes,]
summary(similar.na.passengers$avg.fare)
data_combined[is.na(data_combined$avg.fare), ]
# Get records for similar passengers and summarize avg.fares
indexes <- with(data_combined, which(pclass == "3" & title == "Mr." & family_size == 1 &
ticket != "3701"))
similar.na.passengers <- data_combined[indexes,]
summary(similar.na.passengers$avg.fare)
# Use median since close to mean and a little higher than mean
data_combined[is.na(avg.fare), "avg.fare"] <- 7.840
preproc.data.combined <- data_combined[, c("ticket.party.size", "avg.fare")]
preProc <- preProcess(preproc.data.combined, method = c("center", "scale"))
View(preProc)
postproc.data.combined <- predict(preProc, preproc.data.combined) # "Use the preProc way of preprocessing on preproc.data.combined"
# Hypothesis refuted for all data
cor(postproc.data.combined$ticket.party.size, postproc.data.combined$avg.fare)
# How about for just 1st class all-up?
indexes <- which(data_combined$pclass == "1")
cor(postproc.data.combined$ticket.party.size[indexes],
postproc.data.combined$avg.fare[indexes])
# Hypothesis refuted again
# OK, let's see if our feature engineering has made any difference
features <- c("pclass", "new.title", "family_size", "ticket.party.size", "avg.fare")
rpart.train.3 <- data.combined[1:891, features]
# Run CV and check out results
rpart.3.cv.1 <- rpart.cv(94622, rpart.train.3, RF_LABELS, ctrl_3)
rpart.3.cv.1
# Plot
prp(rpart.3.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
features <- c("pclass", "new.title", "family_size", "ticket.party.size", "avg.fare")
rpart.train.3 <- data_combined[1:891, features]
# Run CV and check out results
rpart.3.cv.1 <- rpart.cv(94622, rpart.train.3, RF_LABELS, ctrl_3)
rpart.3.cv.1
# Plot
prp(rpart.3.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
features <- c("pclass", "new.title", "ticket.party.size", "avg.fare")
rpart.train.4 <- data_combined[1:891, features]
# Run CV and check out results
rpart.4.cv.1 <- rpart.cv(94622, rpart.train.4, RF_LABELS, ctrl_3)
rpart.4.cv.1
# Plot
prp(rpart.4.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
features <- c("pclass", "new.title", "avg.fare")
rpart.train.4 <- data_combined[1:891, features]
# Run CV and check out results
rpart.4.cv.1 <- rpart.cv(94622, rpart.train.4, RF_LABELS, ctrl_3)
rpart.4.cv.1
# Plot
prp(rpart.4.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
features <- c("pclass", "new.title", "ticket.party.size", "avg.fare")
rpart.train.4 <- data_combined[1:891, features]
# Run CV and check out results
rpart.4.cv.1 <- rpart.cv(94622, rpart.train.4, RF_LABELS, ctrl_3)
rpart.4.cv.1
# Plot
prp(rpart.4.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
features <- c("pclass", "new.title", "ticket.party.size")
rpart.train.6 <- data_combined[1:891, features]
# Run CV and check out results
rpart.6.cv.1 <- rpart.cv(94622, rpart.train.6, RF_LABELS, ctrl_3)
rpart.6.cv.1
# Plot
prp(rpart.6.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
rf_train_5_v2 <- data_combined[1:891, c("pclass", "new.title", "ticket.party.size", "avg.fare")]
cluster_1 <- makeCluster(6, type = "SOCK")
registerDoSNOW(cluster_1)
set.seed(237656)
rf_5_cv_3_v2 <- train(x = rf_train_5_v2, y = RF_LABELS, method = "rf", tuneLength = 3,
ntree = 1000, trControl = ctrl_3)
stopCluster(cluster_1)
rf_5_cv_3_v2
test_submit_dataframe <- data_combined[892:1309, c("pclass", "new.title", "ticket.party.size", "avg.fare")]
random_forest_5_v2_predictions <- predict(random_forest_5, test_submit_dataframe)
table(random_forest_5_v2_predictions)
submit_dataframe <- data.frame(PassengerId = rep(892:1309), Survived = random_forest_5_v2_predictions)
write.csv(submit_dataframe, file = "RF_SUB_20201704_1.csv", row.names = FALSE)
rf_5_cv_3_v2
random_forest_5_v2_predictions <- predict(rf_5_cv_3_v2, test_submit_dataframe)
table(random_forest_5_v2_predictions)
submit_dataframe <- data.frame(PassengerId = rep(892:1309), Survived = random_forest_5_v2_predictions)
write.csv(submit_dataframe, file = "RF_SUB_20201704_1.csv", row.names = FALSE)
ctrl_1 <- trainControl(method = "repeatedcv", number = 10, repeats = 10,
index = cv_10_folds)
set.seed(2348)
cv_10_folds <- createMultiFolds(RF_LABELS, k = 10, times = 10)
#Check stratification
table(RF_LABELS)
342 / 549
table(RF_LABELS[cv_10_folds[[33]]])
308 / 494
# Set up caret's trainControl object per above.
ctrl_1 <- trainControl(method = "repeatedcv", number = 10, repeats = 10,
index = cv_10_folds)
rf_train_5_v3 <- data_combined[1:891, c("pclass", "new.title", "ticket.party.size", "avg.fare")]
cluster_1 <- makeCluster(6, type = "SOCK")
registerDoSNOW(cluster_1)
set.seed(828282)
rf_5_cv_10_v3 <- train(x = rf_train_5_v3, y = RF_LABELS, method = "rf", tuneLength = 3,
ntree = 1000, trControl = ctrl_1)
stopCluster(cluster_1)
rf_5_cv_3_v2
test_submit_dataframe <- data_combined[892:1309, c("pclass", "new.title", "ticket.party.size", "avg.fare")]
random_forest_5_v3_predictions <- predict(rf_5_cv_10_v3, test_submit_dataframe)
table(random_forest_5_v3_predictions)
submit_dataframe <- data.frame(PassengerId = rep(892:1309), Survived = random_forest_5_v3_predictions)
write.csv(submit_dataframe, file = "RF_SUB_20201704_2.csv", row.names = FALSE)
