ggplot(misses_updated_1, aes(x = age, fill = as.factor(misses_ph))) +
geom_density(alpha = 0.5) +
xlab("Age") +
ylab("Total Count") +
labs(fill = "Traveling:") +
ggtitle("Misses separation")
####
#If one focuses only on the part 20y.o. and less one can see that our misses_alone is a decent heuristic
#for separating younger women. Good proportion of graph indicates that there are many younger women who are not traveling alone!
#Yes, there are other misses who could travel with their friends etc. but I will stick to this.
###
# - - - sibsp - - -
#####
#Move on to the sibsp variable, summarize the variable
summary(data_combined$sibsp)
####
#One can see that median value is 0 which means that 50% of the passangers are traveling without sibling(s) or spouse
#that mean is so close to the mean which means (pun intended) tendency towards 0 and that the maximum value is 8.
#Nothing so special in my opinion, and can't conclude any meaningful stuff from this yet.
#Maybe we should treat it as a factor if there is a reasonable sense number of distinct values for sibsp.
#That would imensly help us to make some visualizations.
####
length(unique(data_combined$sibsp))
#7 is of a reasonable size so we will indeed treat it as factor.
data_combined$sibsp <- as.factor(data_combined$sibsp)
####
#From here we can make many graphs and get many different analysis of what seems resonable from an analysis perspective.
#One can make analysis relative to any or collection of features.
#I don't want to get into rabbit hole concluding that for example maybe it doesn't seem reasonable to look sibsp relative to name in terms of survival rate.
#Maybe it will help us to extract some features (combination of name and sibsp variable) in the future, but for now I will spare myself of that.
#I think for example that fare and sibsp have some good correlation and including both features would be redundant.
#Since title feature encompasses both sex and age maybe it makes more sense to watch it relative to that, and maybe segment it ("facet wrap it") relative to pclass.
ggplot(data_combined[1:891,], aes(x = sibsp, fill = survived)) +
geom_bar() +
facet_wrap(~pclass + title) +
ggtitle("Pclass, Title") +
xlab("SibSp") +
ylab("Total Count") +
ylim(0,300) +
labs(fill = "Survived")
####
#There are predominantly people who travel with 0 or 1 sibsp. That's what summary on sibsp confirmed!
#Those who travel without sibling or spouse always have the best survival rate.
#Women and children first holds water here.
#Rich folks survived the most holds the water too.
#I don't see too much signal here. Might come back for revision!
####
####
#Not gonna get into too much detail of why it might be intuitive to treat parch as factor.
#At least for the sake of visualization it is possible to make some conclusions if parch is factor.
###
data_combined$parch <- as.factor(data_combined$parch)
ggplot(data_combined[1:891,], aes(x = parch, fill = survived)) +
geom_bar() +
facet_wrap(~pclass + title) +
ggtitle("Pclass, Title") +
xlab("ParCh") +
ylab("Total Count") +
ylim(0,300) +
labs(fill = "Survived")
####
#If you switch in R between plots Pclass, Title vs Sibsp and Pclass, Title vs Parch you can intuit that
#graphs are pretty similar! I'm gonna argue (I keep switching between them while I'm typing this!) that
#survival rates for Mr. is the same in first class when sibs=0, but it's quite worse for the ones in second and third class
#even if data here is a bit skewed toward parch = 0.
#Speaking relatively to sibsp = 0 one can see that number of those who survived is the same in parch = 0,
#but those who perished is increased!
#Maybe someone can conclude something out of it.
#I might be wrong, but I think making model on parch will yield worse results for males than by using sibsp.
####
###
#But maybe if we combine them we will get a feature that is much more expresive!
#Combination of those we will call 'family-size'
###
temp_sibsp <- c(train_set$sibsp, test_set$sibsp) # Since we made
temp_parch <- c(train_set$parch, test_set$parch) #              these two to be factors, and now we need numbers!
data_combined$family_size <- as.factor(temp_sibsp + temp_parch + 1) #If parch and sibsp are factors...
####
#Again, won't argue too much why it might lead us astray to have some connections between name variable for example and new feature we engineered.
####
ggplot(data_combined[1:891,], aes(x = family_size, fill = survived)) +
geom_bar() +
facet_wrap(~pclass + title) +
ggtitle("Pclass, Title") +
xlab("family.size") +
ylab("Total Count") +
ylim(0,300) +
labs(fill = "Survived")
###
#It's much informative in term of "survival trend" among the classes and titles, and maybe even obvious that
#if you have large family and it might be problem for you to keep everyone around, thus the chances of survival are bad.
###
# - - - ticket - - -
#####
#Just before we start I want to emphasise something called overfiting.
#I would be precise and say "overfiting on training data" since we are building our model based on training data.
#We could make a model that could be 100% precise, for example "if you are that and that name, you survive/perish" and build a model on top of that.
#On top of 'name' variables. When we imput some new data (test data) and there is no data with that name in the training set we are ruined.
#That is why we need a model that GENERALIZES on all training data. So when you see those graphs that map each data point precisely, that is the sign of overfitting.
#####
####
#On to the ticket variable. As Dave pointed out, one should get used to write str() for each variable whenever we start analyzing it.
#It's a shorthand for using str() on a combined dataset.
####
str(data_combined$ticket)
####
#Based on the huge number of levels ticket really isn't a factor variable it is a string, so let's convert it.
#Then we will show first 50 of them. We could use 'any number'. It's for a sake of giving us a bigger picture aka we will better see pattern
#if there are relatively high number of data points, especially for this kind of variable.
####
data_combined$ticket <- as.character(data_combined$ticket)
data_combined$ticket[1:50]
####
#There's no immediately apparent structure in the data, let's see if we can find some. I saw on the internet that those indicate something but won't delve deep into that.
#We'll start with taking a look at just the first char for each.
#It's easy for visualization if there are relatively few (we can make them as factor!)
#It's not necessarily correct way of extracting feature, or that one should expect something out of it.
#This is some 'basic heuristic'.
####
ticket_first_char <- ifelse(data_combined$ticket == "", " ", substr(data_combined$ticket,1,1))
unique(ticket_first_char)
####
#OK, we can make a factor for analysis purposes and visualize
###
data_combined$ticket_first_char <- as.factor(ticket_first_char)
# First, a high-level plot of the data
ggplot(data_combined[1:891,], aes(x = ticket_first_char, fill = survived)) +
geom_bar() +
ggtitle("Survivability by ticket.first.char") +
xlab("ticket.first.char") +
ylab("Total Count") +
ylim(0,350) +
labs(fill = "Survived")
####
#One might conclude from the plot that since we know that those tickets that start with 1, 2 and 3 might be indicators of pclass.
#But there are more people in first class that in second:
table(data_combined$pclass)
#Maybe those from 4, 5, etc. could be also grouped into first, second and third class in order to confirm previous hypothesis?
#It's quite a mixed bag of survivability, and we hate overfiting... Might be predictive, but let's drill a bit more!
####
ggplot(data_combined[1:891,], aes(x = ticket_first_char, fill = survived)) +
geom_bar() +
facet_wrap(~pclass) +
ggtitle("Pclass") +
xlab("ticket.first.char") +
ylab("Total Count") +
ylim(0,300) +
labs(fill = "Survived")
###
#We can see that majority tickets in each class start with the same number as pclass they are in.
#If we put that stack of P on top of 1 in the first class we will have "that amount of people" which
#further implies that our previous hypothesis is true! That majority of imbalance for first class is "hidding" in the P?
#There is W in each class, and very few of them. Might indicate some kind of people that are indeed passengers, but maybe some workers or something?
#There might be some signal here, but I will keep it in back of my head.
###
####
# Lastly, see if we get a pattern when using combination of pclass & title
###
ggplot(data_combined[1:891,], aes(x = ticket_first_char, fill = survived)) +
geom_bar() +
facet_wrap(~pclass + title) +
ggtitle("Pclass, Title") +
xlab("ticket.first.char") +
ylab("Total Count") +
ylim(0,200) +
labs(fill = "Survived")
###
#Again, this is based just on a frist letter of ticket. Everything matches with our intuition about survivability, but I don't
#see some patterns that stick out here. Maybe thorough investigation of ticket feature will be more fruitful.
###
# - - - fare - - -
####
#Next up - the fares Titanic passengers paid
####
str(data_combined$fare)
summary(data_combined$fare)
length(unique(data_combined$fare))
###
#We can't make it a factor... Too much instances.
###
###High level overview
ggplot(data_combined, aes(x = fare)) +
geom_histogram(binwidth = 5) +
ggtitle("Combined Fare Distribution") +
xlab("Fare") +
ylab("Total Count") +
ylim(0,200)
####
#There are some folks that haven't paid anything? That could be interesting.
#We see that distribution is skewed towards higher end. We should expect that, because majority of folks that were on Titanic were in third class.
#aka low fares. We confirmed that with median<mean, and also median = 14.454.
#There is an outlier up there where fare>500, but further investigation of fare variable will tell us why is that so.
###
####
#Now that we have high level overview, let's see if it's predictive in some sense:
####
ggplot(data_combined[1:891,], aes(x = fare, fill = survived)) +
geom_histogram(binwidth = 5) +
facet_wrap(~pclass + title) +
ggtitle("Pclass, Title") +
xlab("fare") +
ylab("Total Count") +
ylim(0,50) +
labs(fill = "Survived")
####
#One would expect that rich folks survived more, and indeed that is so. I don't see anything that will cause more than overfitting here, because
#everytihing matches our previous intuitions in terms of survivability. I will leave fare for now, but it might help me to feature engineer something using it.
####
################################################################################################
#                               EXPLANATORY DATA ANALYSIS                                      #
################################################################################################
####
#
#This is the part where we check our intuitions about features, and also test if our feature engineering is worthwhile.
#I think "all" of the ML algos have explicit or implicit way of providing us with the feature importance.
#We need something that is fast (for classification problems!), effective and simple to interpret.
#Without worrying too much on hyperparameters. We will leave that fine tuning for the "real" modeling part.
#
#
#We will use Random Forests here. I wont drill about the algorithm here there is plenty of it on Internet.
#In an essence it uses ensemble method of trees and averages loss on each tree.
#Each tree gets it bootstraped sample (drawing N samples from training set where N is number of rows in training set).
#By drawing samples with replacement some of the rows won't be sampled. Those are colled out-of-bag.
#Evaluate model on samples that you drawed, test the model on non-sampled (out-of-bag) ones.
#What confuses most people is when you are evaluating loss on that particular tree OOB sample is HIDDING it's labels.
#You get the predicted ones, then you compare it with the real labels and calculate loss.
#Draw samples -> Make model (tree) on them -> Test the model (tree) on OOB by hiding it's labels -> Compare real labels of OOB with predicted ones -> Calculate loss
#Then you repeat that proces for each tree and compute the average loss.
#Now watch out, since you are drawing samples RANDOMLY with replacement some of the OOB's from the first tree won't be OOB on some other trees.
#That's why validation on test data is way more accurate of overall accuracy than OOB since each data point from test set has been put
#through EACH TREE in Random Forest, where some of the OOB samples are not tested on all trees.
#There is also (I guess!) a high probability that EACH training sample will be picked in some of the OOB's at least once, so that's why in confusion matrix you will get score for each row.
#
#That's a high-overview enough to get you started. I'm not an expert on RF's...
#
####
###
#Let's import the randomForest library.
###
library(randomForest)
####
#We will use whole dataset but different features for each Random Forest algorithm.
#Note that on combined dataset labels are "1" "0" and "None", where in train dataset we already have preprocessed ones and zeroes.
####
RF_LABELS <- as.factor(train_set$survived) #Made it upper-case since it's constant.
####
#We said that pclass and title are by far the best predictors. By using idea of Occam's Razor we will train our model just on two features.
#set.seed() is used for reproducability. Let's just say that it helps drawing the same samples for trees each time we run algo,
#and that everything that it's varying are the features we use to train RF model.
#
#We set importance to TRUE in order to make feature importance explicit, and ntree is... well I wont philosophise too much about that.
#500 trees is by default. Probably no matter how big your training data is or how many features you use after 500 trees accuracy pretty much plateus.
#And developers decide to make it 500 by default.
#I think the OOB will be a bit accurate when we use more trees since there is high probability that your OOB samples would be
#introudced in more trees, but that's just intuition. It might be wrong. And, since RFs are fast few more trees wont ruin time metric here.
####
rf_train_1 <- data_combined[1:891, c("pclass", "title")]
set.seed(1234)
random_forest_1 <- randomForest(x = rf_train_1, y = RF_LABELS, importance = TRUE, ntree = 1000)
random_forest_1
varImpPlot(random_forest_1) #This one is for PLOTing feature (VARiable) IMPortance
#####
#So our OOB accuracy is 20.99% which is not bad. Don't get into a trap thinking that this will match Kaggle accuracy.
#Put a test set into this model and get a submission to Kaggle.
#We see that this model is good at predicting "pesimistic results" since class.error for those who perish is around 0.024, but bad at predicting optimistic ones.
#We need to have in mind that we must minimize both of these errors. Kaggle gives both accuracies the same weight, that is
#it cares for percentage of good predicted for those who perish, and for those who survive.
#Since (here) upper part of confusion matrix represents Predicted values, and horizontal line represents actuall values class.error is calculated as
#13/536+13 and 174/174+168, which is respectively false positive rate, and true positive rate (sensitivity/recall).
#Intuitevely, if we want to improve overall class error one of the strategy would be to put as much 13s into 536, and 174s into 168s.
#That comes with a price! I think those kind of manipulations are what ROC is preaching: "sacrifising something for something else".
#####
####
#We saw that sibsp, parch and family size might be predictable let's see.
###
####
#Train a Random Forest using pclass, title, & sibsp
###
rf_train_2 <- data_combined[1:891, c("pclass", "title", "sibsp")]
set.seed(1234)
random_forest_2 <- randomForest(x = rf_train_2, y = RF_LABELS, importance = TRUE, ntree = 1000)
random_forest_2
varImpPlot(random_forest_2)
####
#Not only that OOB improved, but also overall error rate! It droped on predicting pessimistic results, but "vastly" improved on predicting ones who survived.
#Better than random_forest_1.
#One of the things I should've mentioned is that by drawing plots of variable importance you see which ones are good, and which ones don't add too much signal.
#There is no point of comparing two features, but it seems more meaningful for more than two!
####
####
#Train a Random Forest using pclass, title, & parch
####
rf_train_3 <- data_combined[1:891, c("pclass", "title", "parch")]
set.seed(1234)
random_forest_3 <- randomForest(x = rf_train_3, y = RF_LABELS, importance = TRUE, ntree = 1000)
random_forest_3
varImpPlot(random_forest_3)
###
#One can see that this one has slightly worse OOB than that on sibsp. I would argue that sibsp and parch are similar in nature, and that some
#values of sibsp is making RF model better. Intuitevely, maybe there is something in the number of siblings or spouses that gives you higher chance of determining surivalism
#then by having parent or children. Maybe there is some advantage you can think of: "children are hard to get together especially if you have more of them, parent's are older and thus have less velocity to get to the upper decks"
#but something grinds my gears since I don't know if random forest intuits this the same way. I'll definetly put some numbers and expect from random forest meaning of life.
#Would it be a happy choice? Satre doesn't thinks so...
###
#####
#Train a Random Forest using pclass, title, sibsp, parch.
#Maybe when we use them in aggregate the RF will intuit groups and see that those with bigger families tend not to survive?
#####
rf_train_4 <- data_combined[1:891, c("pclass", "title", "sibsp", "parch")]
set.seed(1234)
random_forest_4 <- randomForest(x = rf_train_4, y = RF_LABELS, importance = TRUE, ntree = 1000)
random_forest_4
varImpPlot(random_forest_4)
####
#Well, seems like our intuition that ML algos pick humane intuitions validate here. The OOB is by far the lowest, and class errors are good.
#Since ML algos can "maybe" intuit some things, we will assume that it needs aggregation of sibsp and parch to get a better feeling for "familyness".
#There is also that part "traveling all alone". Maybe algo doesn't understand what is the total family size.
####
####
#Train a Random Forest using pclass, title, & family.size
####
rf_train_5 <- data_combined[1:891, c("pclass", "title", "family_size")]
set.seed(1234)
random_forest_5 <- randomForest(x = rf_train_5, y = RF_LABELS, importance = TRUE, ntree = 1000)
random_forest_5
varImpPlot(random_forest_5)
####
#So this gives even better results! We explicitly engineered feature family_size and we see that it helps.
#Now we will see if that is so, let's also add parch and sibsp separately, and then parch and sibsp together.
####
####
#Train a Random Forest using pclass, title, sibsp, & family_size
####
rf_train_6 <- data_combined[1:891, c("pclass", "title", "sibsp", "family_size")]
set.seed(1234)
random_forest_6 <- randomForest(x = rf_train_6, y = RF_LABELS, importance = TRUE, ntree = 1000)
random_forest_6
varImpPlot(random_forest_6)
####
#It got a little worst, but as our intuition is that sibsp is more predictive than parch, let's see if that holds the water!
####
#####
#Train a Random Forest using pclass, title, parch, & family_size
#####
rf_train_7 <- data_combined[1:891, c("pclass", "title", "parch", "family_size")]
set.seed(1234)
random_forest_7 <- randomForest(x = rf_train_7, y = RF_LABELS, importance = TRUE, ntree = 1000)
random_forest_7
varImpPlot(random_forest_7)
####
#Yeah, that might be true.
#Lastly:
####
#####
#Train a Random Forest using pclass, title, parch, sibsp & family_size
#####
rf_train_8 <- data_combined[1:891, c("pclass", "title", "sibsp", "parch", "family_size")]
set.seed(1234)
random_forest_8 <- randomForest(x = rf_train_8, y = RF_LABELS, importance = TRUE, ntree = 1000)
random_forest_8
varImpPlot(random_forest_8)
####
#Maybe our intuition that RF catches the same one is not necessarily good, and we should prefer simpler model.
#So far the best results were yielded using random_forest_5.
####
################################################################################################
#                                      CROSS VALIDATION                                        #
################################################################################################
####
#So, the main idea of this part will be to explain a bit what CV is, and why do we leverage it.
#In an essence it gives almost "real" estimate of what one should expect in terms of accuracy of model.
#Here we will prove that OOB is not indeed our true estimate of accuracy. We explained why way up there.
#Let's put our submission to Kaggle:
test_submit_dataframe <- data_combined[892:1309, c("pclass", "title", "family_size")]
####
#These are the features we used to train random_forest_5. Now, we will use our RF model to predict values on never before seen data.
#How good are those predictions will be based on Kaggles estimate.
####
random_forest_5_predictions <- predict(random_forest_5, test_submit_dataframe)
table(random_forest_5_predictions)
####
#As we pointed out elsewhere, we might expect such result to "predict" more of those who perished.
#We are just checking how much it predicted.
####
####
#Write out a CSV file for submission to Kaggle. Needs to be same format as Kaggle's.
#If you view in R submit_dataframe you will see there are "extra" row namings, hence row.names = FALSE.
####
submit_dataframe <- data.frame(PassengerId = rep(892:1309), Survived = random_forest_5_predictions)
write.csv(submit_dataframe, file = "RF_SUB_20200904_1.csv", row.names = FALSE)
####
#It's pretty obvious how to upload your submission to Kaggle, so I'm gonna skip that part.
#My submission score is 0.79425 but the OOB predicts that we should score 0.8182. We overfitted the training data.
####
####
#Here, caret package uses stratified cross validation.
#The main idea is a methodology of spliting the data, and iteratively training and testing on all training dataset.
#Everyone suggestes that 10 fold cross validation repeated 10 times is first step to any ML modeling.
#We will start with that. Instead of constantly puting submissions to Kaggle and checking score, and the fact you only have 5 sumbissions per day
#we need to find some methodology to calibrate our results localy. We use CV for that. The main idea is to calibrate our score of random_forest_5
#to the point of Kaggle's prediction for this score, that is 0.79426. Everything else is shooting in dark.
#
#As Dave puts:
#"When Kaggle competitions close the final results are calculated using a private data set that is different than either
#the training and test data that is provided publicly. Given that you submit only your predictions to the Kaggle web site,
#Kaggle cannot know what features you used to build your model. While I've never researched how exactly Kaggle does this, there has to be a model(s) that they use that translate the patterns of your model's performance on the test data set to performance on the private data."
####
####
#To leverage the CV methodology we will use doSnow for parallel processing, and caret for creating folds.
####
library(caret)
library(doSNOW)
####
#We will start with the 10-fold-stratified-CV, repeated 10 times. To put it simply: "Shuffle the training set and make me 10 stratified folds, each fold containing 1/10 of the training data.
#Stratified means that each fold is ballanced and to have "same number" of those who survived and those who perished.
#Imagine that you have 9 folds for training where the 10th one is only with those who survived.
#If you are a bit towards math you will see that 9 separate folds with same percentage of survived/perished will in aggregation yield same percentage.
#Why should 10th one be with the same percentage? Well, that one which is used in the holdout set will be included in the training in the next 9 folds, so we have to keep it the same percentage.
#Enough philosophy. If you are curious you can search the Internet for what satisfies your inner "I want to understand."
#I just want to mention that stratification is used for making sure that there are no "hidden features in data", thats why we need to balance it.
#Repeated 10 times will induce even more randomization aka "trying all the possible ways of making test and training data, to ensure higher accuracy"
####
set.seed(2348)
cv_10_folds <- createMultiFolds(RF_LABELS, k = 10, times = 10)
#Check stratification
table(RF_LABELS)
342 / 549
table(RF_LABELS[cv_10_folds[[33]]])
308 / 494
# Set up caret's trainControl object per above.
ctrl_1 <- trainControl(method = "repeatedcv", number = 10, repeats = 10,
index = cv_10_folds)
####
#At this stage check my detecting_cores.r file I've included to check how many aviable cores and threads you have to use.
#I'm not responsible for any damage.
###
###
#Create cluster of child processes, and "register" them
###
cluster_1 <- makeCluster(6, type = "SOCK")
registerDoSNOW(cluster_1)
###
#Set seed for reproducability and train RF algorithm on the same dataset, with the same features as random_forest_5 via using cross-validation
###
set.seed(34324)
rf_5_cv_1 <- train(x = rf_train_5, y = RF_LABELS, method = "rf", tuneLength = 3,
ntree = 1000, trControl = ctrl_1)
###
#Shutdown cluster after it finishes
###
stopCluster(cluster_1)
###
#Check out results
###
rf_5_cv_1
###
#We see that the best accuracy is using mtry=2. that is 0.8105179. Our random_forest_5 accuracy is 81.82, and our Kaggle accuracy
#is 0.79425. So, our CV score is a bit more optimistic, and we see that RF is also overfitting.
#We are training on 90% of data and evaluating our test on 10% of data. Maybe the problem is in having too much data for our RF, aka RF is more prone to overfit then on less data.
#That beign said, we will try to train our RF on less data using 5-fold CV repeated 10 times.
#Same code again, I won't drain it.
###
####
#5-folds cross-validation
####
set.seed(5983)
cv_5_folds <- createMultiFolds(RF_LABELS, k = 5, times = 10)
ctrl_2 <- trainControl(method = "repeatedcv", number = 5, repeats = 10,
index = cv_5_folds)
cluster_1 <- makeCluster(6, type = "SOCK")
registerDoSNOW(cluster_1)
set.seed(89472)
rf_5_cv_2 <- train(x = rf_train_5, y = RF_LABELS, method = "rf", tuneLength = 3,
ntree = 1000, trControl = ctrl_2)
stopCluster(cluster_1)
rf_5_cv_2
###
#Now our best accuracy is 0.8134652 which is quite "worse" then those obtained from 10-fold CV.
#We expected it to be more pessimistic. So finally, we will use the heuristic which is used by some Kagglers:
#Make such CV where folds mimics percentage of the training vs test set on complete dataset.
#Here is around 1/3, that is 2/3 is training set and 1/3 is test set. So, we will do 3-fold cross validation.
###
####
#3-folds cross-validation
####
set.seed(37596)
cv_3_folds <- createMultiFolds(RF_LABELS, k = 3, times = 10)
ctrl_3 <- trainControl(method = "repeatedcv", number = 3, repeats = 10,
index = cv_5_folds)
cluster_1 <- makeCluster(6, type = "SOCK")
registerDoSNOW(cluster_1)
set.seed(94622)
rf_5_cv_3 <- train(x = rf_train_5, y = RF_LABELS, method = "rf", tuneLength = 3,
ntree = 1000, trControl = ctrl_3)
stopCluster(cluster_1)
rf_5_cv_3
####
#As we want to make
set.seed(37596)
cv_3_folds <- createMultiFolds(RF_LABELS, k = 3, times = 10)
ctrl_3 <- trainControl(method = "repeatedcv", number = 3, repeats = 10,
index = cv_3_folds)
cluster_1 <- makeCluster(6, type = "SOCK")
registerDoSNOW(cluster_1)
set.seed(94622)
rf_5_cv_3 <- train(x = rf_train_5, y = RF_LABELS, method = "rf", tuneLength = 3,
ntree = 1000, trControl = ctrl_3)
stopCluster(cluster_1)
rf_5_cv_3
0.8139169 - 0.79425
library(rpart)
library(rpart.plot)
